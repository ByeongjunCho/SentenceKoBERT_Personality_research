{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'sentence-transformers'...\n",
      "remote: Enumerating objects: 4679, done.\u001b[K\n",
      "remote: Counting objects: 100% (224/224), done.\u001b[K\n",
      "remote: Compressing objects: 100% (166/166), done.\u001b[K\n",
      "remote: Total 4679 (delta 114), reused 145 (delta 58), pack-reused 4455\u001b[K\n",
      "Receiving objects: 100% (4679/4679), 15.97 MiB | 17.34 MiB/s, done.\n",
      "Resolving deltas: 100% (3209/3209), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/UKPLab/sentence-transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "sys.path.append(\"../KoBERT\")\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "from gluonnlp.data import SentencepieceTokenizer\n",
    "from kobert.utils import get_tokenizer\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model, Tokenizer load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n",
      "using cached model\n",
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"load model and tokenizer\")\n",
    "# model, vocab load\n",
    "model, vocab  = get_pytorch_kobert_model()\n",
    "\n",
    "# tokenizer load\n",
    "tok_path = get_tokenizer()\n",
    "sp  = SentencepieceTokenizer(tok_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습 데이터 load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "import logging\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Read AllNLI train dataset\")\n",
    "\n",
    "label2int = {\"contradiction\": 0, \"entailment\": 1, \"neutral\": 2}\n",
    "train_samples = []\n",
    "\n",
    "train_df = pd.read_csv('../data/KorNLUDatasets/KorNLI/snli_1.0_train.ko.tsv', sep='\\t', encoding=\"utf-8\")\n",
    "\n",
    "for s1, s2, labels in zip(train_df['sentence1'], train_df['sentence2'], train_df['gold_label']):\n",
    "    label = label2int[labels.strip()]\n",
    "    train_samples.append(InputExample(texts=[s1, s2], label=label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.token_to_idx['[SEP]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLI Dataset 선언\n",
    "class SentencesDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, examples, tokenizer, vocab, max_seq_length):\n",
    "        '''\n",
    "        examples : List[InputExample]\n",
    "        tokenizer : SentencepieceTokenizer\n",
    "        vocab : vocab module\n",
    "        max_seq_length : max sequence length.\n",
    "        '''\n",
    "        self.examples = examples\n",
    "        self.label_type = torch.long if isinstance(self.examples[0].label, int) else torch.float\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab = vocab\n",
    "        self.max_seq_length = max_seq_length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = dict()\n",
    "        # tokenizing\n",
    "        s1, s2 = [self.tokenizer(text) for text in self.examples[idx].texts]\n",
    "        \n",
    "        # slicing(consider special token)\n",
    "        s1 = s1[:self.max_seq_length-2] if len(s1) > self.max_seq_length-2 else s1\n",
    "        s2 = s2[:self.max_seq_length-2] if len(s2) > self.max_seq_length-2 else s2\n",
    "        \n",
    "        # add special token\n",
    "        s1 = ['[CLS]'] + s1 + ['[SEP]']\n",
    "        s2 = ['[CLS]'] + s2 + ['[SEP]']\n",
    "        \n",
    "        # token to index\n",
    "        s1 = [vocab.token_to_idx[x] for x in s1]\n",
    "        s2 = [vocab.token_to_idx[x] for x in s2]\n",
    "        \n",
    "        # add padding\n",
    "        s1_ = s1 + [0]*(self.max_seq_length - len(s1)) if len(s1) < self.max_seq_length else s1[:self.max_seq_length]\n",
    "        s2_ = s2 + [0]*(self.max_seq_length - len(s2)) if len(s2) < self.max_seq_length else s2[:self.max_seq_length]\n",
    "        \n",
    "        item['input_ids1'] = torch.tensor(s1_)\n",
    "        item['input_ids2'] = torch.tensor(s2_)\n",
    "        \n",
    "        # attention mask \n",
    "        attention_mask1 = [1] * len(s1) + [0] * (self.max_seq_length - len(s1)) if len(s1) < self.max_seq_length else [1] * self.max_seq_length\n",
    "        attention_mask2 = [1] * len(s2) + [0] * (self.max_seq_length - len(s2)) if len(s2) < self.max_seq_length else [1] * self.max_seq_length\n",
    "        \n",
    "        item['attention_mask1'] = torch.tensor(attention_mask1)\n",
    "        item['attention_mask2'] = torch.tensor(attention_mask2)\n",
    "        \n",
    "        # label\n",
    "        item['label'] = torch.tensor(self.examples[idx].label, dtype=self.label_type) \n",
    "        \n",
    "        item['token_type_ids1'] = torch.tensor(self.max_seq_length * [0])\n",
    "        item['token_type_ids2'] = torch.tensor(self.max_seq_length * [0])\n",
    "        return item\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids1': tensor([   2, 1962, 4707, 2589,  993, 7178, 5663, 2545, 3554, 1868, 6964, 6115,\n",
       "         5782,   54,    3,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0]),\n",
       " 'input_ids2': tensor([   2, 4955, 2589,  970, 7088, 3567, 1962, 5184, 6723, 3862,   54,    3,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0]),\n",
       " 'attention_mask1': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0]),\n",
       " 'attention_mask2': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0]),\n",
       " 'label': tensor(2),\n",
       " 'token_type_ids1': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0]),\n",
       " 'token_type_ids2': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = SentencesDataset(train_samples, sp, vocab, 30)\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 2\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=train_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids1': tensor([[   2, 4501, 6083, 6538,  517, 6601, 7478, 6983, 4799, 6016, 6538, 2207,\n",
       "          6273, 7329, 3843, 4073, 3313, 7096, 3454, 7078,  745, 4425, 3006, 6093,\n",
       "          7748, 4636, 6116, 1802, 4955,    3],\n",
       "         [   2, 2586, 4303, 5330, 5760, 2369, 3552, 6896, 1773, 2038,  976, 7096,\n",
       "          2718, 3862,   54,    3,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0]]),\n",
       " 'input_ids2': tensor([[   2, 4073, 3313, 7086, 1495, 2207, 6273, 7318, 6896, 4799, 6016,  517,\n",
       "          6601, 7478, 6116, 3838, 3862,    3,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0],\n",
       "         [   2, 1487, 7096, 1958, 7788, 3873,    3,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0]]),\n",
       " 'attention_mask1': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0]]),\n",
       " 'attention_mask2': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0]]),\n",
       " 'label': tensor([0, 1]),\n",
       " 'token_type_ids1': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0]]),\n",
       " 'token_type_ids2': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SoftmaxLoss(NLI datasets)\n",
    "class SoftmaxLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Reference\n",
    "    https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/losses/SoftmaxLoss.py\n",
    "    \n",
    "    This loss was used in our SBERT publication (https://arxiv.org/abs/1908.10084) to train the SentenceTransformer\n",
    "    model on NLI data. It adds a softmax classifier on top of the output of two transformer networks.\n",
    "\n",
    "    :param model: pretrained model\n",
    "    :param sentence_embedding_dimension: Dimension of your sentence embeddings\n",
    "    :param num_labels: Number of different labels\n",
    "    :param concatenation_sent_rep: Concatenate vectors u,v for the softmax classifier?\n",
    "    :param concatenation_sent_difference: Add abs(u-v) for the softmax classifier?\n",
    "    :param concatenation_sent_multiplication: Add u*v for the softmax classifier?\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, model, sentence_embedding_dimension, num_labels, concatenation_sent_rep, concatenation_sent_difference, concatenation_sent_multiplication):\n",
    "        super(SoftmaxLoss, self).__init__()\n",
    "        self.model = model\n",
    "        self.num_labels = num_labels\n",
    "        self.concatenation_sent_rep = concatenation_sent_rep\n",
    "        self.concatenation_sent_difference = concatenation_sent_difference\n",
    "        self.concatenation_sent_multiplication = concatenation_sent_multiplication\n",
    "        \n",
    "        num_vectors_concatenated = 0\n",
    "        if concatenation_sent_rep:\n",
    "            num_vectors_concatenated += 2\n",
    "        if concatenation_sent_difference:\n",
    "            num_vectors_concatenated += 1\n",
    "        if concatenation_sent_multiplication:\n",
    "            num_vectors_concatenated += 1\n",
    "        logging.info(\"Softmax loss: #Vectors concatenated: {}\".format(num_vectors_concatenated))\n",
    "        self.classifier = torch.nn.Linear(num_vectors_concatenated * sentence_embedding_dimension, num_labels)\n",
    "    \n",
    "    def mean_pooling(self, token_embeddings, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        return sum_embeddings / sum_mask\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # input_ids1, attention_mask1, token_type_ids1, input_ids2, attention_mask2, token_type_ids2, labels\n",
    "        output1 = self.model(input_ids=x['input_ids1'], attention_mask=x['attention_mask1'], token_type_ids=x['token_type_ids1'])\n",
    "        output2 = self.model(input_ids=x['input_ids2'], attention_mask=x['attention_mask2'], token_type_ids=x['token_type_ids2'])\n",
    "        \n",
    "        rep_a = self.mean_pooling(output1[0], x['attention_mask1'])\n",
    "        rep_b = self.mean_pooling(output2[0], x['attention_mask2'])\n",
    "        \n",
    "        vectors_concat = []\n",
    "        if self.concatenation_sent_rep:\n",
    "            vectors_concat.append(rep_a)\n",
    "            vectors_concat.append(rep_b)\n",
    "\n",
    "        if self.concatenation_sent_difference:\n",
    "            vectors_concat.append(torch.abs(rep_a - rep_b))\n",
    "\n",
    "        if self.concatenation_sent_multiplication:\n",
    "            vectors_concat.append(rep_a * rep_b)\n",
    "\n",
    "        features = torch.cat(vectors_concat, 1)\n",
    "        \n",
    "        output = self.classifier(features)\n",
    "        loss_fct = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        if labels is not None:\n",
    "            loss = loss_fct(output, x['label'].view(-1))\n",
    "            return loss\n",
    "        else:\n",
    "            return reps, output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLI 학습 및 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLI dataset load\n",
    "logging.info(\"Read AllNLI train dataset\")\n",
    "\n",
    "label2int = {\"contradiction\": 0, \"entailment\": 1, \"neutral\": 2}\n",
    "train_samples = []\n",
    "\n",
    "train_df = pd.read_csv('../data/KorNLUDatasets/KorNLI/snli_1.0_train.ko.tsv', sep='\\t', encoding=\"utf-8\")\n",
    "\n",
    "for s1, s2, labels in zip(train_df['sentence1'], train_df['sentence2'], train_df['gold_label']):\n",
    "    label = label2int[labels.strip()]\n",
    "    train_samples.append(InputExample(texts=[s1, s2], label=label))\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "max_sequence_length = 64\n",
    "\n",
    "# train dataset Load\n",
    "train_dataset = SentencesDataset(train_samples, sp, vocab, max_sequence_length)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=train_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation dataset load\n",
    "logging.info(\"Read STSbenchmark dev dataset\")\n",
    "dev_samples = []\n",
    "\n",
    "with open('../data/KorNLUDatasets/KorSTS/tune_dev.tsv', 'rt', encoding='utf-8') as fIn:\n",
    "    lines = fIn.readlines()\n",
    "    for line in lines:\n",
    "        s1, s2, score = line.split('\\t')\n",
    "        score = score.strip()\n",
    "        score = float(score) / 5.0\n",
    "        dev_samples.append(InputExample(texts= [s1,s2], label=score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dataset = SentencesDataset(dev_samples, sp, vocab, max_sequence_length)\n",
    "dev_dataloader = DataLoader(dev_dataset, shuffle=True, batch_size=train_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a4d924ab9a84a59ab86f8a186f4a8d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   0%|          | 0/275076 [00:00<?, ?it/s]<ipython-input-16-b4f9f52685ca>:51: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  torch.nn.utils.clip_grad_norm(SBERT_model.parameters(), 1)\n",
      "Iteration:   0%|          | 998/275076 [01:30<6:40:36, 11.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 1.0999, epochs: 0, training_steps: 999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   1%|          | 1999/275076 [03:11<8:23:20,  9.04it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 1.0912, epochs: 0, training_steps: 1999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   1%|          | 2999/275076 [04:52<8:24:41,  8.98it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 1.0596, epochs: 0, training_steps: 2999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   1%|▏         | 3999/275076 [06:40<9:47:27,  7.69it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 1.0278, epochs: 0, training_steps: 3999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   2%|▏         | 4999/275076 [08:28<8:05:25,  9.27it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.9953, epochs: 0, training_steps: 4999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   2%|▏         | 5999/275076 [09:59<8:07:15,  9.20it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.9535, epochs: 0, training_steps: 5999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   3%|▎         | 6998/275076 [11:29<6:38:38, 11.21it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.9746, epochs: 0, training_steps: 6999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   3%|▎         | 7998/275076 [12:58<6:33:37, 11.31it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.9439, epochs: 0, training_steps: 7999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   3%|▎         | 8999/275076 [14:48<8:09:44,  9.05it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.9500, epochs: 0, training_steps: 8999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   4%|▎         | 9998/275076 [16:17<6:30:36, 11.31it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.9255, epochs: 0, training_steps: 9999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   4%|▍         | 10998/275076 [17:46<6:29:08, 11.31it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.9039, epochs: 0, training_steps: 10999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   4%|▍         | 11999/275076 [19:16<6:25:50, 11.36it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.9045, epochs: 0, training_steps: 11999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   5%|▍         | 12999/275076 [20:51<7:57:52,  9.14it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.9094, epochs: 0, training_steps: 12999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   5%|▌         | 13998/275076 [22:20<6:24:49, 11.31it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.9272, epochs: 0, training_steps: 13999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   5%|▌         | 14999/275076 [24:05<7:57:24,  9.08it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.9158, epochs: 0, training_steps: 14999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   6%|▌         | 15998/275076 [25:35<6:22:52, 11.28it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.9374, epochs: 0, training_steps: 15999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   6%|▌         | 16998/275076 [27:04<6:21:51, 11.26it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.9024, epochs: 0, training_steps: 16999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   7%|▋         | 17998/275076 [28:34<6:22:23, 11.20it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.9536, epochs: 0, training_steps: 17999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   7%|▋         | 18999/275076 [30:16<9:08:18,  7.78it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.8780, epochs: 0, training_steps: 18999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   7%|▋         | 19999/275076 [32:03<7:44:02,  9.16it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.9483, epochs: 0, training_steps: 19999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   8%|▊         | 20999/275076 [33:52<7:48:43,  9.03it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.9128, epochs: 0, training_steps: 20999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   8%|▊         | 21999/275076 [35:22<6:13:40, 11.29it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.9449, epochs: 0, training_steps: 21999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   8%|▊         | 22999/275076 [37:09<7:40:29,  9.12it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.8920, epochs: 0, training_steps: 22999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   9%|▊         | 23999/275076 [38:41<7:39:09,  9.11it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.9144, epochs: 0, training_steps: 23999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   9%|▉         | 24998/275076 [40:10<6:13:56, 11.15it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.9070, epochs: 0, training_steps: 24999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   9%|▉         | 25999/275076 [41:55<7:36:32,  9.09it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.9151, epochs: 0, training_steps: 25999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  10%|▉         | 26999/275076 [43:30<7:27:29,  9.24it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.9389, epochs: 0, training_steps: 26999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  10%|█         | 27999/275076 [45:00<8:27:46,  8.11it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.9612, epochs: 0, training_steps: 27999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  11%|█         | 28999/275076 [46:47<7:26:40,  9.18it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.9468, epochs: 0, training_steps: 28999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  11%|█         | 29998/275076 [48:17<6:04:20, 11.21it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.9496, epochs: 0, training_steps: 29999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  11%|█▏        | 30999/275076 [49:55<8:08:59,  8.32it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.9310, epochs: 0, training_steps: 30999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  12%|█▏        | 31999/275076 [51:35<6:40:09, 10.12it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.9108, epochs: 0, training_steps: 31999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  12%|█▏        | 32998/275076 [53:15<6:35:28, 10.20it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.9187, epochs: 0, training_steps: 32999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  12%|█▏        | 33998/275076 [54:49<5:52:46, 11.39it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.9352, epochs: 0, training_steps: 33999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  13%|█▎        | 34998/275076 [56:19<5:56:37, 11.22it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.9120, epochs: 0, training_steps: 34999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  13%|█▎        | 35998/275076 [57:48<5:54:19, 11.25it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.9221, epochs: 0, training_steps: 35999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  13%|█▎        | 36999/275076 [59:22<6:39:46,  9.93it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.9276, epochs: 0, training_steps: 36999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  14%|█▍        | 37998/275076 [1:01:01<6:31:41, 10.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.9216, epochs: 0, training_steps: 37999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  14%|█▍        | 38998/275076 [1:02:33<5:49:46, 11.25it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.9001, epochs: 0, training_steps: 38999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  15%|█▍        | 39998/275076 [1:04:02<5:45:11, 11.35it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.9002, epochs: 0, training_steps: 39999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  15%|█▍        | 40998/275076 [1:05:31<5:45:34, 11.29it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.9161, epochs: 0, training_steps: 40999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  15%|█▌        | 41999/275076 [1:07:10<6:53:03,  9.40it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.9026, epochs: 0, training_steps: 41999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  16%|█▌        | 42999/275076 [1:08:59<7:09:46,  9.00it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.9116, epochs: 0, training_steps: 42999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  16%|█▌        | 43998/275076 [1:10:28<5:42:50, 11.23it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.9363, epochs: 0, training_steps: 43999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  16%|█▋        | 44998/275076 [1:11:58<5:37:39, 11.36it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.9124, epochs: 0, training_steps: 44999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  17%|█▋        | 45998/275076 [1:13:27<5:42:51, 11.14it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.9291, epochs: 0, training_steps: 45999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  17%|█▋        | 46998/275076 [1:14:57<5:36:04, 11.31it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.8914, epochs: 0, training_steps: 46999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  17%|█▋        | 47999/275076 [1:16:26<5:33:22, 11.35it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.8941, epochs: 0, training_steps: 47999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  18%|█▊        | 48999/275076 [1:17:57<5:37:23, 11.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.8757, epochs: 0, training_steps: 48999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  18%|█▊        | 49999/275076 [1:19:27<5:38:43, 11.07it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.8928, epochs: 0, training_steps: 49999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  19%|█▊        | 50999/275076 [1:20:56<5:30:01, 11.32it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.8810, epochs: 0, training_steps: 50999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  19%|█▉        | 51999/275076 [1:22:25<5:31:42, 11.21it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.9318, epochs: 0, training_steps: 51999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  19%|█▉        | 52999/275076 [1:23:55<5:25:41, 11.36it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.9271, epochs: 0, training_steps: 52999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  20%|█▉        | 53999/275076 [1:25:24<5:25:41, 11.31it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.9125, epochs: 0, training_steps: 53999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  20%|█▉        | 54999/275076 [1:26:54<5:28:19, 11.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.9251, epochs: 0, training_steps: 54999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  20%|██        | 55999/275076 [1:28:24<6:38:58,  9.15it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.9792, epochs: 0, training_steps: 55999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  21%|██        | 56999/275076 [1:30:18<6:34:32,  9.21it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.9169, epochs: 0, training_steps: 56999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  21%|██        | 57999/275076 [1:31:54<6:34:54,  9.16it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.9149, epochs: 0, training_steps: 57999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  21%|██▏       | 58999/275076 [1:33:43<6:32:44,  9.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.9183, epochs: 0, training_steps: 58999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  22%|██▏       | 59999/275076 [1:35:17<6:36:26,  9.04it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.9104, epochs: 0, training_steps: 59999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  22%|██▏       | 60998/275076 [1:36:46<5:16:39, 11.27it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.9132, epochs: 0, training_steps: 60999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  23%|██▎       | 61999/275076 [1:38:32<7:08:18,  8.29it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.8951, epochs: 0, training_steps: 61999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  23%|██▎       | 62999/275076 [1:40:23<6:57:05,  8.47it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.9041, epochs: 0, training_steps: 62999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  23%|██▎       | 63999/275076 [1:42:06<6:25:04,  9.14it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.8815, epochs: 0, training_steps: 63999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  24%|██▎       | 64999/275076 [1:43:37<6:20:39,  9.20it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.9289, epochs: 0, training_steps: 64999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  24%|██▍       | 65999/275076 [1:45:18<6:24:46,  9.06it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.9204, epochs: 0, training_steps: 65999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  24%|██▍       | 66998/275076 [1:46:47<5:08:40, 11.24it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.9084, epochs: 0, training_steps: 66999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  25%|██▍       | 67998/275076 [1:48:17<5:18:15, 10.84it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.9063, epochs: 0, training_steps: 67999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  25%|██▌       | 68999/275076 [1:50:13<6:51:50,  8.34it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.9097, epochs: 0, training_steps: 68999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  25%|██▌       | 69999/275076 [1:52:02<6:15:12,  9.11it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.8782, epochs: 0, training_steps: 69999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  26%|██▌       | 70998/275076 [1:53:31<5:00:41, 11.31it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.9096, epochs: 0, training_steps: 70999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  26%|██▌       | 71999/275076 [1:55:07<6:12:57,  9.08it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.8794, epochs: 0, training_steps: 71999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  27%|██▋       | 72999/275076 [1:56:56<6:02:36,  9.29it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.8892, epochs: 0, training_steps: 72999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  27%|██▋       | 73999/275076 [1:58:38<6:03:33,  9.22it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.8608, epochs: 0, training_steps: 73999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  27%|██▋       | 74999/275076 [2:00:23<6:02:03,  9.21it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.8838, epochs: 0, training_steps: 74999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  28%|██▊       | 75999/275076 [2:02:05<5:59:42,  9.22it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss : 0.9102, epochs: 0, training_steps: 75999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  28%|██▊       | 76321/275076 [2:02:36<5:19:18, 10.37it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "not a string",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-b4f9f52685ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mtraining_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Iteration'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mSBERT_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mSBERT_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1164\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1165\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1166\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-7b812a48e80c>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# tokenizing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# slicing(consider special token)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-7b812a48e80c>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# tokenizing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# slicing(consider special token)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/gluonnlp/data/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, sample)\u001b[0m\n\u001b[1;32m    558\u001b[0m             \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m         \"\"\"\n\u001b[0;32m--> 560\u001b[0;31m         return self._processor.SampleEncodeAsPieces(sample, self._nbest,\n\u001b[0m\u001b[1;32m    561\u001b[0m                                                     self._alpha)\n\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sentencepiece/__init__.py\u001b[0m in \u001b[0;36mSampleEncodeAsPieces\u001b[0;34m(self, input, nbest_size, alpha)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mSampleEncodeAsPieces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_sentencepiece\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentencePieceProcessor_SampleEncodeAsPieces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mSampleEncodeAsIds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: not a string"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "num_epochs = 1\n",
    "weight_decay = 0.01\n",
    "\n",
    "num_training_steps = int(len(train_dataloader) * num_epochs)\n",
    "\n",
    "import math\n",
    "# model 선언\n",
    "SBERT_model = SoftmaxLoss(model, 768, 3, True, True, False)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "\n",
    "param_optimizer = list(SBERT_model.named_parameters())\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "\n",
    "# optimizer 선언\n",
    "from transformers import AdamW\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5, eps=1e-6, correct_bias=False)\n",
    "warmup_steps = math.ceil(len(train_dataset) * num_epochs / train_batch_size * 0.1) #10% of train data for warm-up\n",
    "scheduler = transformers.get_linear_schedule_with_warmup(optimizer, warmup_steps, num_training_steps=num_training_steps)\n",
    "\n",
    "\n",
    "# NLI dataset 학습\n",
    "from tqdm import tqdm\n",
    "from tqdm.autonotebook import trange\n",
    "\n",
    "# 기록을 위해 tensorboard 사용\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(f'runs/{num_epochs}_{batch_size}')\n",
    "\n",
    "global_steps = 0\n",
    "running_loss = 0\n",
    "for epoch in trange(num_epochs, desc='Epoch', disable=not True):\n",
    "    training_steps = 0\n",
    "    \n",
    "    for batch in tqdm(train_dataloader, desc='Iteration', disable=False):\n",
    "        SBERT_model.zero_grad()\n",
    "        SBERT_model.train()\n",
    "        SBERT_model.to(device)\n",
    "        \n",
    "        batch = {key: batch[key].to(device) for key in batch.keys()}\n",
    "        \n",
    "        loss_value = SBERT_model(batch)\n",
    "        loss_value.backward()\n",
    "        torch.nn.utils.clip_grad_norm(SBERT_model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        running_loss += loss_value.item()\n",
    "        \n",
    "        if training_steps % 1000 == 999:\n",
    "            print(f'training_loss : {running_loss / 1000:.4f}, epochs: {epoch}, training_steps: {training_steps}')\n",
    "            writer.add_scalar('training_loss', running_loss / 1000, training_steps + epoch * len(train_dataloader))\n",
    "            torch.save(SBERT_model, f'./trained_model/{epoch}_{training_steps}_{running_loss/1000:.4f}.pt')\n",
    "            running_loss = 0\n",
    "        \n",
    "        training_steps += 1\n",
    "    global_steps += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(token_embeddings, attention_mask):\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info('STS dataset ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(dev_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
